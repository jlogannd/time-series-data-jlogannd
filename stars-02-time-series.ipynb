{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hideCode": false,
    "hidePrompt": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f12af47cb022924020e918e3f6d16b57",
     "grade": false,
     "grade_id": "instr-headline",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# In March of 2019 there were floods in South Dakota, USA\n",
    "![](https://theintercept.imgix.net/wp-uploads/sites/1/2019/04/h_15196312-Pipeline-Flooding-1554474495-e1554474625282.jpg?auto=compress%2Cformat&q=90&fit=crop&w=1440&h=720)\n",
    "\n",
    "> Image source: <a src=https://theintercept.com/2019/04/05/keystone-xl-pipeline-pine-ridge-floods/> The Intercept April 5, 2019</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa8d05bc217a47d3aa3bde245d83e01b",
     "grade": false,
     "grade_id": "instr-intro",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In March 2019, large parts of South Dakota were flooded for weeks. What happened to cause this flooding? What impacts did the flooding have? Before we look at data about the flooding, we need to check out what other sources are saying about it.\n",
    "\n",
    "&#128214; Here are some resources from different sources to get you started:\n",
    "  * [The National Weather Service](https://www.weather.gov/unr/2019-05-26_31) \n",
    "  * [South Dakota Public Radio](https://listen.sdpb.org/news/2019-10-17cheyenne-river-tribe-says-oahe-dam-has-caused-problems-for-decades)\n",
    "  * [The Intercept](https://theintercept.com/2019/04/05/keystone-xl-pipeline-pine-ridge-floods/)\n",
    "\n",
    "&#128172; If you or someone you know have experience with this site, or \n",
    "were there during the floods, we also invite you to write about that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d2bd1c4945146a088da15dddc1bcc6f",
     "grade": false,
     "grade_id": "task-site",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "\n",
    "## The Cheyenne River near Wasta, SD was one of the locations affected by the flooding\n",
    "\n",
    "To start, you'll be focusing on the Cheyenne River, which flows into \n",
    "Lake Oahu. Then, you'll pick your own site that was affected by a flood.\n",
    "\n",
    "### Site Description\n",
    "\n",
    "&#9998; In the cell below, describe the Cheyenne River area in a few sentences. \n",
    "You can include:\n",
    "  * Information about the **climatology** of the area, or typical \n",
    "  precipitation and temperature at different months of the year\n",
    "  * The **runoff ratio** (average annual runoff divided by average \n",
    "  annual precipitation)\n",
    "  * Which **wildlife and ecosystems** exist in the area\n",
    "  * What **communities and infrastructure** are in the area\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f162bbab15453be957803b2887a9ebdf",
     "grade": false,
     "grade_id": "instr-set-up",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Interactive Site Map\n",
    "\n",
    "#### Get set up to use Python\n",
    "\n",
    "Use the cell below to add necessary **package imports** to this notebook. It's best to import everything in your very first code cell because it helps folks who are reading your code to figure out where everything comes from (mostly right now this is **you** in the future). It's *very* frustrating to try to figure out what packages need to be installed to get some code to run.\n",
    "\n",
    "&#128214; Our friend [the PEP-8 style guide has some things to say about imports](https://peps.python.org/pep-0008/#imports). In particular - **standard library packages** should be listed at the top. These are packages that you don't need to install because they come with Python. You can check if a package is part of the standard library by searching the [Python Standard Library documentation page](https://docs.python.org/3/library/). \n",
    "\n",
    "&#128187; Your task:\n",
    "  * **Uncomment** all the import lines below. HINT: Use the `CMD`-`/` shortcut to uncomment many lines at once.\n",
    "  * Add the **library for working with DataFrames in Python** to the imports\n",
    "  * Separate the **standard library package(s)** at the top\n",
    "  * Run and test your import cell to make sure everything will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18cfa35d60929e03f7e1c1d3026e1a9b",
     "grade": false,
     "grade_id": "student-imports-answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    " import os\n",
    " import pathlib\n",
    " import subprocess\n",
    " from io import BytesIO\n",
    "\n",
    " import folium\n",
    " import matplotlib.pyplot as plt\n",
    " import matplotlib.dates as dates\n",
    " import pandas as pd\n",
    " import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hideCode": false,
    "hidePrompt": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04860dcce448a21a2fc9d14f31042d15",
     "grade": true,
     "grade_id": "student-imports-tests",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test package imports - DO NOT MODIFY THIS CELL!\n",
    "import_answer_points = 3\n",
    "\n",
    "# Check that pandas has been imported properly\n",
    "try:\n",
    "    na_val = pd.NA\n",
    "    print(\"\\u2705 Score! Pandas has been imported as a pd!\")\n",
    "    import_answer_points += 2\n",
    "except NameError:\n",
    "    print(\n",
    "        \"\\u274C Pandas has not been imported as a pd, please make \"\n",
    "        \"sure to import it properly.\"\n",
    "    )\n",
    "\n",
    "# Subtract one point for any PEP-8 errors\n",
    "tmp_path = \"tmp.py\"\n",
    "with open(tmp_path, \"w\") as tmp_file:\n",
    "    tmp_file.write(In[-2])\n",
    "ignore_flake8 = 'W292,F401,E302'\n",
    "flake8_out = subprocess.run(\n",
    "    ['flake8', \n",
    "     '--ignore', ignore_flake8, \n",
    "     '--import-order-style', 'edited',\n",
    "     '--count', \n",
    "     tmp_path],\n",
    "    stdout=subprocess.PIPE,\n",
    ").stdout.decode(\"ascii\")\n",
    "print(flake8_out)\n",
    "import_answer_points -= int(flake8_out.splitlines()[-1])\n",
    "\n",
    "print(\n",
    "    \"\\n \\u27A1 You received {} out of 5 points.\".format(import_answer_points)\n",
    ")\n",
    "\n",
    "import_answer_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4eb914eee3c2bfee3187cbde72af5758",
     "grade": false,
     "grade_id": "task-map",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Site Map: The Cheyenne River near Wasta\n",
    "\n",
    "The code below will create an interactive map of the area using the **folium**\n",
    "library. But something is wrong - no one defined the latitude and longitude as\n",
    "**variables**.\n",
    "\n",
    "&#128187; Your task:\n",
    "  * Find the location of the Cheyenne River near Wasta **USGS stream gauge** using the [National Water Information System](https://waterdata.usgs.gov/nwis?). This is not the easiest thing to find if you aren't used to NWIS, so you can use the following instructions to get started:\n",
    "      * Go to the [National Water Information System Mapper](https://dashboard.waterdata.usgs.gov/app/nwd/en/)\n",
    "      * Type in `Wasta` in the `Find a Place` box\n",
    "      * Click on the Cheyenne River near Wasta site. It should open a new window.\n",
    "      * Click on `Site page` at the top\n",
    "      * Scroll to the bottom and open the `Location metadata` section.\n",
    "  * Define latitude and longitude variables to **match the variable names \n",
    "    used in the code**.\n",
    "  * Change the current label, \"Thingy\" to be descriptive of the site.\n",
    "  * Run and test your cell to make sure everything works.\n",
    "\n",
    "&#127798; EXTRA CHALLENGE: Customize your folium plot [using the folium documentation](https://python-visualization.github.io/folium/quickstart.html#Getting-Started). For example, you could:\n",
    "  * Change the base map images\n",
    "  * Change the initial zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a675b5d94d456541f785078a0459e4fd",
     "grade": false,
     "grade_id": "ans-map",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
       "&lt;html&gt;\n",
       "&lt;head&gt;\n",
       "    \n",
       "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
       "    \n",
       "        &lt;script&gt;\n",
       "            L_NO_TOUCH = false;\n",
       "            L_DISABLE_3D = false;\n",
       "        &lt;/script&gt;\n",
       "    \n",
       "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
       "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://code.jquery.com/jquery-1.12.4.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
       "    \n",
       "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
       "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
       "            &lt;style&gt;\n",
       "                #map_43e7630e2739209bdc5b08e31746dd8b {\n",
       "                    position: relative;\n",
       "                    width: 100.0%;\n",
       "                    height: 100.0%;\n",
       "                    left: 0.0%;\n",
       "                    top: 0.0%;\n",
       "                }\n",
       "                .leaflet-container { font-size: 1rem; }\n",
       "            &lt;/style&gt;\n",
       "        \n",
       "&lt;/head&gt;\n",
       "&lt;body&gt;\n",
       "    \n",
       "    \n",
       "            &lt;div class=&quot;folium-map&quot; id=&quot;map_43e7630e2739209bdc5b08e31746dd8b&quot; &gt;&lt;/div&gt;\n",
       "        \n",
       "&lt;/body&gt;\n",
       "&lt;script&gt;\n",
       "    \n",
       "    \n",
       "            var map_43e7630e2739209bdc5b08e31746dd8b = L.map(\n",
       "                &quot;map_43e7630e2739209bdc5b08e31746dd8b&quot;,\n",
       "                {\n",
       "                    center: [46.7032765, -92.4188018],\n",
       "                    crs: L.CRS.EPSG3857,\n",
       "                    zoom: 10,\n",
       "                    zoomControl: true,\n",
       "                    preferCanvas: false,\n",
       "                    scrollWheelZoom: false,\n",
       "                }\n",
       "            );\n",
       "\n",
       "            \n",
       "\n",
       "        \n",
       "    \n",
       "            var tile_layer_e8e9449d6bcd275c56fd21d0120a6fed = L.tileLayer(\n",
       "                &quot;https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
       "                {&quot;attribution&quot;: &quot;Data by \\u0026copy; \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://openstreetmap.org\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e, under \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://www.openstreetmap.org/copyright\\&quot;\\u003eODbL\\u003c/a\\u003e.&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
       "            ).addTo(map_43e7630e2739209bdc5b08e31746dd8b);\n",
       "        \n",
       "    \n",
       "            var marker_377c23d90b9e4c84be4fc24ec5e2ce57 = L.marker(\n",
       "                [46.7032765, -92.4188018],\n",
       "                {}\n",
       "            ).addTo(map_43e7630e2739209bdc5b08e31746dd8b);\n",
       "        \n",
       "    \n",
       "        var popup_43ffda2daa58d1b921709a00b62f09c8 = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_727b8402a2b55efc612b6f7ff69bc559 = $(`&lt;div id=&quot;html_727b8402a2b55efc612b6f7ff69bc559&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Scanlon, MN&lt;/div&gt;`)[0];\n",
       "                popup_43ffda2daa58d1b921709a00b62f09c8.setContent(html_727b8402a2b55efc612b6f7ff69bc559);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_377c23d90b9e4c84be4fc24ec5e2ce57.bindPopup(popup_43ffda2daa58d1b921709a00b62f09c8)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "&lt;/script&gt;\n",
       "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x7f16d5a48820>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#WASTA, SD\n",
    "#sg_lat = 44.08109849\n",
    "#sg_lon = -102.4012746\n",
    "\n",
    "#St. Louis River in Duluth, MN\n",
    "sg_lat = 46.7032765\n",
    "sg_lon = -92.4188018\n",
    "\n",
    "# Initialize map and tweak settings\n",
    "m = folium.Map(\n",
    "    # Location to display\n",
    "    location=(sg_lat, sg_lon),\n",
    "    # Turns off annoying zooming while trying to scroll to the next cell\n",
    "    scrollWheelZoom=False)\n",
    "\n",
    "# Put a marker at the stream gauge location\n",
    "#folium.Marker([sg_lat, sg_lon], popup=\"Wasta, SD\").add_to(m)\n",
    "folium.Marker([sg_lat, sg_lon], popup=\"Scanlon, MN\").add_to(m)\n",
    "\n",
    "# Display the map\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08e9042f6da954ce7c26c6016c227795",
     "grade": false,
     "grade_id": "instr-floods",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## One way to express how big a flood is by estimating how often larger floods occur.\n",
    "\n",
    "For example, you might have heard news media talking about a \"100-year flood\". \n",
    "\n",
    "In this notebook, you will write Python code to download and work with a **time series** of streamflow data during the flooding on the Cheyenne River.\n",
    "\n",
    "> A **time series** of data is taken at the same location but collected regularly or semi-regularly over time. \n",
    "\n",
    "You will then consider how the values compared to previous years before the flood event by computing the flood's **return period**.\n",
    "\n",
    "> A **return period** is an estimate of how often you might expect to see a flood of at least a particular size. This does *NOT* mean an extreme flood \"has\" to occur within the return period, or that it couldn't occur more than once.\n",
    "\n",
    "&#128214; Here are some resources from your text book you can review to learn more:\n",
    "  * [Introduction to time-series data](https://www.earthdatascience.org/courses/use-data-open-source-python/use-time-series-data-in-python/)\n",
    "  * [Flood return period and probability](https://www.earthdatascience.org/courses/use-data-open-source-python/use-time-series-data-in-python/floods-return-period-and-probability/)\n",
    "\n",
    "&#9998; In the cell below, explain what data you will need to complete this analysis, including:\n",
    "  1. What type or types of data do you need?\n",
    "  2. How many years of data do you think you need to compute the return period of an extreme event like the 2019 Cheyenne River floods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "13a0749b01486be67d82e42ab14e2b60",
     "grade": false,
     "grade_id": "task-url",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### US streamflow data are available from the National Water Information Service (NWIS) \n",
    "\n",
    "&#128187; Practice downloading the data you need using the NWIS website. **You will not use your downloaded data in the analysis, but you must follow these steps to get the correct urls.** In the cell below, use the following instructions to get urls for downloading the USGS data:\n",
    "\n",
    "1. Go back to the Cheyenne River near Wasta station page.\n",
    "4. This time, click `Data` instead of `Site Page`\n",
    "4. Select `Daily Data` from the list of datasets.\n",
    "5. Select the entire available date range, and set your results to be as `Tab-separated`, and press `Go`.\n",
    "6. Copy the url that populates in your browser window and paste it below. You don't need to save the data - we will do that using Python.\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9998; USGS streamflow URL: \n",
    "Cheyenne River: https://waterdata.usgs.gov/nwis/dv?cb_all_=on&cb_00060=on&cb_00065=on&format=rdb&site_no=06423500&legacy=&referred_module=sw&period=&begin_date=2022-05-24&end_date=2023-05-24\n",
    "\n",
    "St. Louis River: https://waterdata.usgs.gov/nwis/dv?cb_00010=on&cb_00060=on&cb_00095=on&cb_00300=on&cb_00400=on&cb_63680=on&format=rdb&site_no=04024000&legacy=&referred_module=sw&period=&begin_date=1914-01-01&end_date=2023-06-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "85aef85b9260339c6c6329541f4fcb8f",
     "grade": false,
     "grade_id": "task-api",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### Exploring the NWIS API\n",
    "\n",
    "One way to access data is through an **Application Programming Interface**, or **API**. The URL you've just found is an example of a simple, public API. All the parameters of your data search are visible in the URL. For example, to get data starting in 1950, we could change `begin_date=1914-10-01` to `begin_date=1950-01-01`)\n",
    "\n",
    " &#9998; In the cell below - what parameter would you change in the USGS url if you wanted to switch locations?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf988eff04d79ba72e82650476bab754",
     "grade": false,
     "grade_id": "task-citation",
     "locked": true,
     "points": 12,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### Data description and citation\n",
    "\n",
    "&#9998; In the cell below, describe your data. Include the following information:\n",
    "  1. A 1-2 sentence description of the data\n",
    "  2. Data citation\n",
    "  3. What are the units?\n",
    "  4. What is the time interval for each data point?\n",
    "  5. Is there a \"no data\" value, or a value used to indicate when the sensor was broken or didn't detect anything? (These are also known as NA, N/A, NaN, nan, or nodata values)\n",
    "\n",
    "&#128214; The [NWIS data format page](https://waterdata.usgs.gov/nwis/?tab_delimited_format_info) might be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hideCode": false,
    "hidePrompt": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "78d2b95246caa72725ed4c8a7b1d7109",
     "grade": false,
     "grade_id": "set-working-directory-instructions",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Download the data\n",
    "\n",
    "In the cell below complete the following task:\n",
    "\n",
    "1. Replace the empty string `''` in the code below with the USGS NWIS URL you found, saving it in the `nwis_url` variable.\n",
    "2. Download the data using the provided code.\n",
    "3. Save the result (or HTTP Response) to a **descriptive variable**, and call the variable at the end of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "hideCode": false,
    "hidePrompt": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89f19cb74c17578a2d9e7f325b69fe4c",
     "grade": false,
     "grade_id": "download-and-set-working-directory",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Cheyenne River\n",
    "#nwis_url = 'https://waterdata.usgs.gov/nwis/dv?cb_all_=on&cb_00060=on&cb_00065=on&format=rdb&site_no=06423500&legacy=&referred_module=sw&period=&begin_date=1950-01-01&end_date=2023-06-07'\n",
    "\n",
    "#St. Louis River\n",
    "nwis_url = 'https://waterdata.usgs.gov/nwis/dv?cb_00010=on&cb_00060=on&cb_00095=on&cb_00300=on&cb_00400=on&cb_63680=on&format=rdb&site_no=04024000&legacy=&referred_module=sw&period=&begin_date=1914-01-01&end_date=2023-06-12'\n",
    "\n",
    "\n",
    "# Download data using a GET HTTP Request\n",
    "nwis_data = requests.get(nwis_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "967a5f95747eb0ce83c8f4355d432239",
     "grade": true,
     "grade_id": "cell-7dccd7491268de2d",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'ok'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4435/4005803474.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mans_req\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mreq_pts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mans_req\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\u2705 Great work! Your download succeeded'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mreq_pts\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5985\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5986\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'ok'"
     ]
    }
   ],
   "source": [
    "ans_req = _\n",
    "req_pts = 0\n",
    "\n",
    "if ans_req.ok:\n",
    "    print('\\u2705 Great work! Your download succeeded')\n",
    "    req_pts +=2\n",
    "else:\n",
    "    print('\\u274C Hmm, looks like your url is not correct')\n",
    "\n",
    "print('\\u27A1 You earned {} of 2 points for downloading data'.format(req_pts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5342cc4ad725bee71f5e03541780874c",
     "grade": false,
     "grade_id": "instr-look-at-data",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### You will need to take a look at the raw downloaded data to figure out what import parameters to use with the pandas read_csv() function\n",
    "\n",
    "&#128187; In the cell below, replace `response` with the name of the response variable that you defined above.\n",
    "\n",
    "The code below prints the first 10 lines of your download and numbers them. Does this look like streamflow data to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 b'# ---------------------------------- WARNING ----------------------------------------'\n",
      "1 b'# Some of the data that you have obtained from this U.S. Geological Survey database'\n",
      "2 b\"# may not have received Director's approval. Any such data values are qualified\"\n",
      "3 b'# as provisional and are subject to revision. Provisional data are released on the'\n",
      "4 b'# condition that neither the USGS nor the United States Government may be held liable'\n",
      "5 b'# for any damages resulting from its use.'\n",
      "6 b'#'\n",
      "7 b'# Additional info: https://help.waterdata.usgs.gov/policies/provisional-data-statement'\n",
      "8 b'#'\n",
      "9 b'# File-format description:  https://help.waterdata.usgs.gov/faq/about-tab-delimited-output'\n"
     ]
    }
   ],
   "source": [
    "for i, line in enumerate(nwis_data.content.splitlines()[:10]):\n",
    "    print(i, line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "91818f0cb841ee907383b85ab86fb4bc",
     "grade": false,
     "grade_id": "instr-comment",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the [NWIS documentation](https://waterdata.usgs.gov/nwis/?tab_delimited_format_info), they say that you can ignore lines that start with a hash sign (#) because they are **commented**. When we use pandas to import the data, we'll be able to tell it what character indicates a comment, but we're not there yet. The code below again prints the first 35 lines of the response content, this time skipping all commented lines. \n",
    "\n",
    "&#128187; In the cell below, replace `response` with the name of the response variable that you defined above. Then run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the data. What got downloaded?\n",
    "for i, line in enumerate(nwis_data.content.splitlines()[:35]):\n",
    "    if not line.startswith(b'#'):\n",
    "        print(i, line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5511456e9ebcb95423d212ccb7d64d1e",
     "grade": false,
     "grade_id": "instr-describe-data",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "&#9998; What do you notice about the data now? In the following cell, write down your thoughts on:\n",
    "  * What separator or **delimiter** does the data use to separate columns?\n",
    "  * What should the data types of each column be?\n",
    "  * Which column contains the streamflow data?\n",
    "  * Do you need to skip any rows that don't contain data?\n",
    "  * Which column do you think makes sense as the **index** (unique identifier) for each row?\n",
    "  * Is there anything else strange?\n",
    "\n",
    "The answers to the questions above will help you figure out what parameters to use with the `pd.read_csv()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "159f43019e7201e37afb31552c80099a",
     "grade": false,
     "grade_id": "instr-import",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Now we're ready to import the data with pandas. \n",
    "\n",
    "Notice that when you print your downloaded data, each line has a `b` in front of it. The `b` stands for \"bytes\". In order for pandas to be able to read the data, we need to **decode** it so each line is a regular string. In the cell below, we do this using the `io.BytesIO` function, which tricks `pandas` into thinking it is reading a binary file.\n",
    "\n",
    "&#128187; Your task:\n",
    "  * Replace `response` with the name of your HTTP Response variable\n",
    "  * Uncomment the code below, **one line at a time**.\n",
    "  * Using the observations you made above, add the necessary values to get `pandas` to correctly import the data.\n",
    "  * Make sure to include units in your column names where applicable! What units are these streamflow measurements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c9d8a331827383133ef3717021ae919",
     "grade": false,
     "grade_id": "ans-import",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1965/1294047005.py:1: DtypeWarning: Columns (1,3,4,5,6,7,8,9,11,13,15,17,19,21,23,24,25,26,27,28,29,30,31,32,33,34) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  river_flow = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "river_flow = pd.read_csv(\n",
    "    BytesIO(nwis_data.content),\n",
    "    comment='#',\n",
    "    delimiter='\\t', \n",
    "    #skiprows=[30, 31],\n",
    "    #names=['Agency', 'Site Number', 'Date', 'Discharge', 'Site Code', 'Site Code 2', 'Site Code 3'],\n",
    "    #index_col='Date',\n",
    "    #parse_dates=True,\n",
    ")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#river_flow_update = river_flow.drop(columns = ['Site Code 2', 'Site Code 3'])\n",
    "#river_flow_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d3d62d6bf0061c526654692b359c0c6",
     "grade": true,
     "grade_id": "test-import",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ans_df = _\n",
    "df_points = 0\n",
    "\n",
    "if len(ans_df) >= 39658:\n",
    "    print(\"\\u2705 Looks like your DataFrame has enough rows!\")\n",
    "    df_points += 2\n",
    "else:\n",
    "    print(\"\\u274C Oops, your DataFrame doesnt have enough rows\")\n",
    "\n",
    "if len(ans_df.columns) == 4:\n",
    "    print(\"\\u2705 Looks like your DataFrame has enough columns!\")\n",
    "    df_points += 2\n",
    "elif len(ans_df.columns) == 5:\n",
    "    print(\"\\u274C Hmm, looks like you didn't set an index column\")\n",
    "else:\n",
    "    print(\"\\u274C Oops, your DataFrame doesn't have the right number of \"\n",
    "          \"columns\")\n",
    "    \n",
    "print(\"\\u27A1 You earned {} of 4 points\".format(df_points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "617086bc7f7c06fd9aafd66587d7000e",
     "grade": false,
     "grade_id": "instr-type",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's check your data. A useful method for looking at the **datatypes** in your `pd.DataFrame` is the `pd.DataFrame.info()` method.\n",
    "\n",
    "> In Python, you will see both **methods** and **functions**. This is an *important and tricky* distinction we'll be talking about a lot. For right now -- functions have all of their arguments/parameters **inside** the parentheses, as in `pd.read_csv(args)`. For **methods**, the first argument is always some kind of Python **object** like a `pd.DataFrame`. Take a look at the next cell for an example of using the `pd.DataFrame.info()` **method**.\n",
    "\n",
    "\n",
    "&#128187;  Replace `dataframe` with the name of your DataFrame variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "05e1e29fde8cbf00122e190bdb880fe4",
     "grade": false,
     "grade_id": "task-type",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Oops, we have one more problem! Take a look at the data types of your `DataFrame` columns...\n",
    "\n",
    "&#9998; In the cell below, write down what data type you would expect the streamflow column to be. The main options are: Integer, Float, Datetime, or Object.\n",
    "\n",
    "&#128214; Check out [this example showing the most common data types for pandas columns](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dtypes.html)\n",
    "\n",
    "> A **float** is a non-integer number. You can identify them because they have decimal points in Python, unlike integers. We do not call them **decimals** for a reason - a `decimal.Decimal` is different, and more precise than, a `float` in Python. If you are ever working with really, really small numbers, you may need to use **decimals**, but for most applications floats are fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b8bcc57cc8d10293fe2266e561096e7",
     "grade": false,
     "grade_id": "cell-afe83558de537192",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "`pandas` was able to apply the correct data type to some columns, but not to the streamflow column. One reason this happens is because there are some values in the `DataFrame` that cannot be read in or **parsed** as the same data type as everything else. Often, these are **no data values**. Unfortunately, the [documentation](https://waterdata.usgs.gov/nwis/?tab_delimited_format_info) does not list any no data values.\n",
    "\n",
    "The code below runs through the values in the streamflow column one by one. It **tries** to convert each value to a **float**, but if it fails it prints the result and then stops.\n",
    "\n",
    "> Q is a common variable name for streamflow in hydrology\n",
    "\n",
    "&#128187; Replace `dataframe` below with your `DataFrame` name, and `streamflow_cfs` with your streamflow column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in dataframe.streamflow_cfs:\n",
    "    try: \n",
    "        float(q)\n",
    "    except:\n",
    "        print(q)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "834e9d15097e30aa3c80f9bf248bf67d",
     "grade": false,
     "grade_id": "instr-import2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Looks like some of the streamflow data is a string instead of a number. This lets us know that no data could be taken that day because the Cheyenne River was frozen! We can let Python know that there isn't any data there using the `na_values='...'` parameter. Substitute the value you found for the `...`\n",
    "\n",
    "&#128187; Re-import your data below, this time indicating an NA value. Call your new `DataFrame` at the end for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9945cf32d89c4c78b31b57ca4d994f0c",
     "grade": false,
     "grade_id": "ans-import2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "333acca25af39e2bdd4f113566669ad0",
     "grade": true,
     "grade_id": "test-import2",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ans_q = _\n",
    "q_points = 0\n",
    "\n",
    "if isinstance(ans_q, pd.DataFrame):\n",
    "    print(\"\\u2705 Great, you created a pandas dataframe above\")\n",
    "    q_points += 1\n",
    "else:\n",
    "    print(\"\\u274C Oops - the cell above should have a DataFrame output.\")\n",
    "\n",
    "if type(ans_q.index) == pd.DatetimeIndex:\n",
    "    print(\"\\u2705 Your DataFrame has the date as the index, \"\n",
    "          \"good job!\")\n",
    "    q_points += 1\n",
    "else:\n",
    "    print(\"\\u274C Your DataFrame does not have the date \"\n",
    "          \"as the index.\")\n",
    "    \n",
    "import numpy as np\n",
    "if ans_q.iloc[:,2].dtype == np.float64:\n",
    "    print(\"\\u2705 Your streamflow column is floats!\")\n",
    "    q_points += 2\n",
    "else:\n",
    "    print(\"\\u274C Your streamflow column still isn't floats.\")\n",
    "\n",
    "if round(ans_q.iloc[:,2].mean(), 0)==385:\n",
    "    print(\"\\u2705 Your streamflow DataFrame has the expected values \"\n",
    "          \"in it, good job!\")\n",
    "    q_points += 2\n",
    "else:\n",
    "    print(\"\\u274C Your streamflow DataFrame does not have the \"\n",
    "          \"expected values in it.\")\n",
    "\n",
    "print(\"\\u27A1 You received {} out of 6 points for opening the \"\n",
    "      \"streamflow data.\".format(\n",
    "    q_points))\n",
    "q_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3aa0edd6dc1ba49fb77a3f1c784b8e99",
     "grade": false,
     "grade_id": "discharge-subset-instructions",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Can we see the flood in the streamflow data?\n",
    "\n",
    "In the cell below, subset the stream discharge data to the same timeframe that you are interested in: February - April, 2019. Save the result to a variable and call it at the end of the cell for testing.\n",
    "\n",
    "You can find some [examples of subsetting time series data in the textbook](https://www.earthdatascience.org/courses/use-data-open-source-python/use-time-series-data-in-python/date-time-types-in-pandas-python/subset-time-series-data-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89f9f8ea1c69bde63c116ab0a6f1318c",
     "grade": false,
     "grade_id": "discharge-daily",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d0411f16bfc00e565ecc75a7a76662a",
     "grade": true,
     "grade_id": "test-subset",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ans_subset = _\n",
    "subset_points = 0\n",
    "\n",
    "# Answer should be a DataFrame\n",
    "if isinstance(ans_subset, pd.DataFrame):\n",
    "    print(\"\\u2705 Great, you created a pandas dataframe above\")\n",
    "    subset_points += 1\n",
    "else:\n",
    "    print(\"\\u274C Oops - the cell above should have a DataFrame output.\")\n",
    "\n",
    "# Answer should have a Datetime index\n",
    "if type(ans_subset.index) == pd.DatetimeIndex:\n",
    "    print(\"\\u2705 Your DataFrame has the date as the index, \"\n",
    "          \"good job!\")\n",
    "    subset_points += 1\n",
    "else:\n",
    "    print(\"\\u274C Your DataFrame does not have the date \"\n",
    "          \"as the index.\")\n",
    "\n",
    "# Answer should include 89 days of data\n",
    "if len(ans_subset)==89:\n",
    "    print(\"\\u2705 Your DataFrame has the right number of days\")\n",
    "    subset_points += 2\n",
    "elif len(ans_subset) > 89:\n",
    "    print(\"\\u274C Your subset has too many days.\")\n",
    "else:\n",
    "    print(\"\\u274C Your subset has too few days.\")\n",
    "\n",
    "# The mean of the streamflow column should be 1951\n",
    "if round(ans_subset.iloc[:,2].mean(), 0)==1951:\n",
    "    print(\"\\u2705 Your streamflow DataFrame has the expected values \"\n",
    "          \"in it, good job!\")\n",
    "    subset_points += 1\n",
    "else:\n",
    "    print(\"\\u274C Your streamflow DataFrame does not have the \"\n",
    "          \"expected values in it.\")\n",
    "\n",
    "print(\"\\u27A1 You received {} out of 5 points for subsetting the \"\n",
    "      \"streamflow data.\".format(\n",
    "    subset_points))\n",
    "subset_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d50ee6e2b5f04629e5694389a34eef37",
     "grade": false,
     "grade_id": "task-plot-subset",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "&#128187; Now, in the cell below, plot your subsetted data. Don't forget to label your plot!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f66eefd035a0cad068455450501375b",
     "grade": false,
     "grade_id": "ans-plot-subset",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "76e580278bf5ba53523a189bf3793008",
     "grade": false,
     "grade_id": "task-daily-plot",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "You should be able to see the flood in your data going up above 12000 cfs at its peak. But how unusual is that really?\n",
    "\n",
    "Let's start by plotting ALL the data. Then we'll use a return period **statistic** to quantify how unusual it was.\n",
    "\n",
    "&#128187; In the cell below, plot the entire time series of streamflow data, without any parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "203aa09940282bb04b9b273eeaa9079c",
     "grade": false,
     "grade_id": "ans-daily-plot",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef5055e03894e124e66244fd0583ee91",
     "grade": false,
     "grade_id": "instr-resample",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This plot looks a little fuzzy because it is trying to fit too many data points in a small area. One way to improve this is by **resampling** the data to **annual maxima**. That way we still get the same peak streamflows, but the computer will be able to plot all the values without overlapping.\n",
    "\n",
    "> **Resampling** means changing the time interval between time series observations - in this case from daily to annual.\n",
    "\n",
    "&#128214; Read about [different ways to resample time series data in your textbook](https://www.earthdatascience.org/courses/use-data-open-source-python/use-time-series-data-in-python/date-time-types-in-pandas-python/resample-time-series-data-pandas-python/)\n",
    "\n",
    "&#128214; You can use a [list of **offset aliases**](https://pandas.pydata.org/docs/dev/user_guide/timeseries.html#timeseries-offset-aliases) to look up how to specify the final dates. This list is pretty hard to find - you might want to bookmark it.\n",
    "\n",
    "&#128187; In the cell below, select the streamflow column, and then resample it to get an annual maximum.\n",
    "\n",
    "> Watch out for this gotcha - the test below is looking for a pandas `DataFrame`, but when we select a single column we get a pandas `Series` (a `DataFrame` is a collection of `Series`.) To get a `DataFrame` with a single column, use the syntax below with **two** square brackets:\n",
    "\n",
    "```python\n",
    "dataframe[['column_name']]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c529e9bc45291695ef50b098237de18f",
     "grade": false,
     "grade_id": "ans-resample",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce8fba22fcc60edec633d3c03d0175ef",
     "grade": true,
     "grade_id": "test-resample",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ans_resample = _\n",
    "resample_points = 0\n",
    "\n",
    "# Answer should be a DataFrame\n",
    "if isinstance(ans_resample, pd.DataFrame):\n",
    "    print(\"\\u2705 Great, you created a pandas DataFrame above\")\n",
    "    resample_points += 1\n",
    "else:\n",
    "    print(\"\\u274C Oops - the cell above should have a DataFrame output.\")\n",
    "\n",
    "# Answer should have a Datetime index\n",
    "if type(ans_resample.index) == pd.DatetimeIndex:\n",
    "    print(\"\\u2705 Your DataFrame has the date as the index, \"\n",
    "          \"good job!\")\n",
    "    resample_points += 1\n",
    "else:\n",
    "    print(\"\\u274C Your DataFrame does not have the date \"\n",
    "          \"as the index.\")\n",
    "\n",
    "# Answer should include 89 days of data\n",
    "if len(ans_resample)>=110:\n",
    "    print(\"\\u2705 Your DataFrame has the right number of years\")\n",
    "    resample_points += 2\n",
    "else:\n",
    "    print(\"\\u274C Oops - did you resample your DataFrame to annual?\")\n",
    "\n",
    "# The mean of the streamflow Series should be 7888\n",
    "if round(int(ans_resample.mean()), 0)==7888:\n",
    "    print(\"\\u2705 Your annual max streamflow DataFrame has the expected \"\n",
    "          \"values in it, good job!\")\n",
    "    resample_points += 1\n",
    "else:\n",
    "    print(\"\\u274C Your annual max streamflow DataFrame does not have the \"\n",
    "          \"expected values in it.\")\n",
    "\n",
    "print(\"\\u27A1 You received {} out of 5 points for subsetting the \"\n",
    "      \"streamflow data.\".format(\n",
    "    resample_points))\n",
    "resample_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6efa60724edf08e8e7f71ebde2942e11",
     "grade": false,
     "grade_id": "instr-plot-annual",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "&#128187; Plot your resampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fecf4ac619a6a3634f1250450ed77458",
     "grade": false,
     "grade_id": "dicharge-monthly-max",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e25a268a2f33f6e83d0097c74c7f15ef",
     "grade": false,
     "grade_id": "task-describe",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "In the cell below, write a headline and 2-3 sentence description of your plot. What do you estimate the return period was for the flood in 2019?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9100cd4217f18f4016dcd4e322ffa5d",
     "grade": false,
     "grade_id": "task-return",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "&#127798; In the cell below, calculate the exceedence probability and return period for each year of the **annual** data, and add them as columns to your DataFrame.\n",
    "\n",
    "> HINT: pandas columns have a `rank` method, which you can use. BUT -- you will need to use the `ascending=False` parameter, since higher rank should be lower exceedence probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d8cda4b09e90a30e7db8845f304752a",
     "grade": false,
     "grade_id": "ans-return",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52fec71c5fc9dd0af7070316421867a4",
     "grade": true,
     "grade_id": "tests-return",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ans_return = _\n",
    "return_points = 0\n",
    "\n",
    "# Answer should be a DataFrame\n",
    "if isinstance(ans_return, pd.DataFrame):\n",
    "    print(\"\\u2705 Great, you created a pandas dataframe above\")\n",
    "    return_points += 1\n",
    "else:\n",
    "    print(\"\\u274C Oops - the cell above should have a DataFrame output.\")\n",
    "\n",
    "# Answer should have a Datetime index\n",
    "if type(ans_return.index) == pd.DatetimeIndex:\n",
    "    print(\"\\u2705 Your DataFrame has the date as the index, \"\n",
    "          \"good job!\")\n",
    "    return_points += 1\n",
    "else:\n",
    "    print(\"\\u274C Your DataFrame does not have the date \"\n",
    "          \"as the index.\")\n",
    "\n",
    "# Answer should include 110 years of data\n",
    "if len(ans_return)==110:\n",
    "    print(\"\\u2705 Your DataFrame has the right number of days\")\n",
    "    return_points += 2\n",
    "elif len(ans_return) > 110:\n",
    "    print(\"\\u274C Your DataFrame has too many years.\")\n",
    "else:\n",
    "    print(\"\\u274C Your DataFrame has too few years.\")\n",
    "\n",
    "# The value \"hash\" should be 20549.0\n",
    "if round(ans_return.mean().product(), 0)==20549.0:\n",
    "    print(\"\\u2705 Your streamflow DataFrame has the expected values \"\n",
    "          \"in it, good job!\")\n",
    "    return_points += 1\n",
    "else:\n",
    "    print(\"\\u274C Your streamflow DataFrame does not have the \"\n",
    "          \"expected values in it.\")\n",
    "\n",
    "print(\"\\u27A1 You received {} out of 5 extra credit points for calculating the \"\n",
    "      \"return period.\".format(return_points))\n",
    "return_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aef8b057f821b8e8e3dab835b6e98a81",
     "grade": false,
     "grade_id": "pep8-grading",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Pep 8, and Does the Notebook Run?\n",
    "In this cell, we will give you points for the following\n",
    "\n",
    "1. PEP 8 is followed throughout the notebook (3 points)\n",
    "3. The notebook runs from top to bottom without any editing (it is reproducible) (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
       "&lt;html&gt;\n",
       "&lt;head&gt;\n",
       "    \n",
       "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
       "    \n",
       "        &lt;script&gt;\n",
       "            L_NO_TOUCH = false;\n",
       "            L_DISABLE_3D = false;\n",
       "        &lt;/script&gt;\n",
       "    \n",
       "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
       "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://code.jquery.com/jquery-1.12.4.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
       "    \n",
       "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
       "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
       "            &lt;style&gt;\n",
       "                #map_082594ae291937e729bd6d74f7a79f8d {\n",
       "                    position: relative;\n",
       "                    width: 100.0%;\n",
       "                    height: 100.0%;\n",
       "                    left: 0.0%;\n",
       "                    top: 0.0%;\n",
       "                }\n",
       "                .leaflet-container { font-size: 1rem; }\n",
       "            &lt;/style&gt;\n",
       "        \n",
       "&lt;/head&gt;\n",
       "&lt;body&gt;\n",
       "    \n",
       "    \n",
       "            &lt;div class=&quot;folium-map&quot; id=&quot;map_082594ae291937e729bd6d74f7a79f8d&quot; &gt;&lt;/div&gt;\n",
       "        \n",
       "&lt;/body&gt;\n",
       "&lt;script&gt;\n",
       "    \n",
       "    \n",
       "            var map_082594ae291937e729bd6d74f7a79f8d = L.map(\n",
       "                &quot;map_082594ae291937e729bd6d74f7a79f8d&quot;,\n",
       "                {\n",
       "                    center: [46.7032765, -92.4188018],\n",
       "                    crs: L.CRS.EPSG3857,\n",
       "                    zoom: 10,\n",
       "                    zoomControl: true,\n",
       "                    preferCanvas: false,\n",
       "                    scrollWheelZoom: false,\n",
       "                }\n",
       "            );\n",
       "\n",
       "            \n",
       "\n",
       "        \n",
       "    \n",
       "            var tile_layer_dc5b153980dec48a3e896f6db918a1d0 = L.tileLayer(\n",
       "                &quot;https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
       "                {&quot;attribution&quot;: &quot;Data by \\u0026copy; \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://openstreetmap.org\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e, under \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://www.openstreetmap.org/copyright\\&quot;\\u003eODbL\\u003c/a\\u003e.&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
       "            ).addTo(map_082594ae291937e729bd6d74f7a79f8d);\n",
       "        \n",
       "    \n",
       "            var marker_75eea46a6d26b54b4eb1ea82cf564350 = L.marker(\n",
       "                [46.7032765, -92.4188018],\n",
       "                {}\n",
       "            ).addTo(map_082594ae291937e729bd6d74f7a79f8d);\n",
       "        \n",
       "    \n",
       "        var popup_31a533546aa2de761b81407b8d5b6e5d = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_e7dad8a09b7d1cb83754390a0a7a501a = $(`&lt;div id=&quot;html_e7dad8a09b7d1cb83754390a0a7a501a&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Scanlon, MN&lt;/div&gt;`)[0];\n",
       "                popup_31a533546aa2de761b81407b8d5b6e5d.setContent(html_e7dad8a09b7d1cb83754390a0a7a501a);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_75eea46a6d26b54b4eb1ea82cf564350.bindPopup(popup_31a533546aa2de761b81407b8d5b6e5d)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "&lt;/script&gt;\n",
       "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x7f16d277ead0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#St. Louis River in Scanlon, MN\n",
    "sg_lat = 46.7032765\n",
    "sg_lon = -92.4188018\n",
    "\n",
    "# Initialize map and tweak settings\n",
    "m = folium.Map(\n",
    "    # Location to display\n",
    "    location=(sg_lat, sg_lon),\n",
    "    # Turns off annoying zooming while trying to scroll to the next cell\n",
    "    scrollWheelZoom=False)\n",
    "\n",
    "# Put a marker at the stream gauge location\n",
    "#folium.Marker([sg_lat, sg_lon], popup=\"Wasta, SD\").add_to(m)\n",
    "folium.Marker([sg_lat, sg_lon], popup=\"Scanlon, MN\").add_to(m)\n",
    "\n",
    "# Display the map\n",
    "m\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 b'# ---------------------------------- WARNING ----------------------------------------'\n",
      "1 b'# Some of the data that you have obtained from this U.S. Geological Survey database'\n",
      "2 b\"# may not have received Director's approval. Any such data values are qualified\"\n",
      "3 b'# as provisional and are subject to revision. Provisional data are released on the'\n",
      "4 b'# condition that neither the USGS nor the United States Government may be held liable'\n",
      "5 b'# for any damages resulting from its use.'\n",
      "6 b'#'\n",
      "7 b'# Additional info: https://help.waterdata.usgs.gov/policies/provisional-data-statement'\n",
      "8 b'#'\n",
      "9 b'# File-format description:  https://help.waterdata.usgs.gov/faq/about-tab-delimited-output'\n"
     ]
    }
   ],
   "source": [
    "#St. Louis River\n",
    "nwis_url = 'https://waterdata.usgs.gov/nwis/dv?cb_00010=on&cb_00060=on&cb_00095=on&cb_00300=on&cb_00400=on&cb_63680=on&format=rdb&site_no=04024000&legacy=&referred_module=sw&period=&begin_date=1914-01-01&end_date=2023-06-12'\n",
    "\n",
    "\n",
    "# Download data using a GET HTTP Request\n",
    "nwis_data = requests.get(nwis_url)\n",
    "\n",
    "for i, line in enumerate(nwis_data.content.splitlines()[:10]):\n",
    "    print(i, line)\n",
    "\t\n",
    "# Take a look at the data. What got downloaded?\n",
    "for i, line in enumerate(nwis_data.content.splitlines()[:35]):\n",
    "    if not line.startswith(b'#'):\n",
    "        print(i, line)\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1965/605042372.py:1: DtypeWarning: Columns (1,3,4,5,6,7,8,9,11,13,15,17,19,21,23,24,25,26,27,28,29,30,31,32,33,34) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  river_flow = pd.read_csv(\n",
      "/tmp/ipykernel_1965/605042372.py:1: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  river_flow = pd.read_csv(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency_cd</th>\n",
       "      <th>site_no</th>\n",
       "      <th>214956_63680_00001</th>\n",
       "      <th>214956_63680_00001_cd</th>\n",
       "      <th>214957_63680_00002</th>\n",
       "      <th>214957_63680_00002_cd</th>\n",
       "      <th>214958_63680_00003</th>\n",
       "      <th>214958_63680_00003_cd</th>\n",
       "      <th>72609_00010_00001</th>\n",
       "      <th>72609_00010_00001_cd</th>\n",
       "      <th>...</th>\n",
       "      <th>72617_00400_00002</th>\n",
       "      <th>72617_00400_00002_cd</th>\n",
       "      <th>72618_00400_00008</th>\n",
       "      <th>72618_00400_00008_cd</th>\n",
       "      <th>72619_00300_00001</th>\n",
       "      <th>72619_00300_00001_cd</th>\n",
       "      <th>72620_00300_00002</th>\n",
       "      <th>72620_00300_00002_cd</th>\n",
       "      <th>72621_00300_00003</th>\n",
       "      <th>72621_00300_00003_cd</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20d</th>\n",
       "      <td>5s</td>\n",
       "      <td>15s</td>\n",
       "      <td>14n</td>\n",
       "      <td>10s</td>\n",
       "      <td>14n</td>\n",
       "      <td>10s</td>\n",
       "      <td>14n</td>\n",
       "      <td>10s</td>\n",
       "      <td>14n</td>\n",
       "      <td>10s</td>\n",
       "      <td>...</td>\n",
       "      <td>14n</td>\n",
       "      <td>10s</td>\n",
       "      <td>14n</td>\n",
       "      <td>10s</td>\n",
       "      <td>14n</td>\n",
       "      <td>10s</td>\n",
       "      <td>14n</td>\n",
       "      <td>10s</td>\n",
       "      <td>14n</td>\n",
       "      <td>10s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914-01-01</th>\n",
       "      <td>USGS</td>\n",
       "      <td>04024000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914-01-02</th>\n",
       "      <td>USGS</td>\n",
       "      <td>04024000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914-01-03</th>\n",
       "      <td>USGS</td>\n",
       "      <td>04024000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914-01-04</th>\n",
       "      <td>USGS</td>\n",
       "      <td>04024000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-08</th>\n",
       "      <td>USGS</td>\n",
       "      <td>4024000</td>\n",
       "      <td>3.2</td>\n",
       "      <td>P</td>\n",
       "      <td>2.0</td>\n",
       "      <td>P</td>\n",
       "      <td>2.3</td>\n",
       "      <td>P</td>\n",
       "      <td>23.8</td>\n",
       "      <td>P</td>\n",
       "      <td>...</td>\n",
       "      <td>7.7</td>\n",
       "      <td>P</td>\n",
       "      <td>7.8</td>\n",
       "      <td>P</td>\n",
       "      <td>7.9</td>\n",
       "      <td>P</td>\n",
       "      <td>6.1</td>\n",
       "      <td>P</td>\n",
       "      <td>7.0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-09</th>\n",
       "      <td>USGS</td>\n",
       "      <td>4024000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>P:[4]</td>\n",
       "      <td>1.9</td>\n",
       "      <td>P:[4]</td>\n",
       "      <td>2.1</td>\n",
       "      <td>P:[4]</td>\n",
       "      <td>22.5</td>\n",
       "      <td>P:[4]</td>\n",
       "      <td>...</td>\n",
       "      <td>7.8</td>\n",
       "      <td>P:[4]</td>\n",
       "      <td>7.9</td>\n",
       "      <td>P:[4]</td>\n",
       "      <td>8.0</td>\n",
       "      <td>P:[4]</td>\n",
       "      <td>6.8</td>\n",
       "      <td>P:[4]</td>\n",
       "      <td>7.4</td>\n",
       "      <td>P:[4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-10</th>\n",
       "      <td>USGS</td>\n",
       "      <td>4024000</td>\n",
       "      <td>2.3</td>\n",
       "      <td>P</td>\n",
       "      <td>1.8</td>\n",
       "      <td>P</td>\n",
       "      <td>2.0</td>\n",
       "      <td>P</td>\n",
       "      <td>22.3</td>\n",
       "      <td>P</td>\n",
       "      <td>...</td>\n",
       "      <td>7.8</td>\n",
       "      <td>P</td>\n",
       "      <td>7.9</td>\n",
       "      <td>P</td>\n",
       "      <td>7.7</td>\n",
       "      <td>P</td>\n",
       "      <td>7.1</td>\n",
       "      <td>P</td>\n",
       "      <td>7.4</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-11</th>\n",
       "      <td>USGS</td>\n",
       "      <td>4024000</td>\n",
       "      <td>2.3</td>\n",
       "      <td>P</td>\n",
       "      <td>1.6</td>\n",
       "      <td>P</td>\n",
       "      <td>1.9</td>\n",
       "      <td>P</td>\n",
       "      <td>22.2</td>\n",
       "      <td>P</td>\n",
       "      <td>...</td>\n",
       "      <td>7.7</td>\n",
       "      <td>P</td>\n",
       "      <td>7.9</td>\n",
       "      <td>P</td>\n",
       "      <td>7.8</td>\n",
       "      <td>P</td>\n",
       "      <td>6.0</td>\n",
       "      <td>P</td>\n",
       "      <td>7.3</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-12</th>\n",
       "      <td>USGS</td>\n",
       "      <td>4024000</td>\n",
       "      <td>2.7</td>\n",
       "      <td>P</td>\n",
       "      <td>1.7</td>\n",
       "      <td>P</td>\n",
       "      <td>2.1</td>\n",
       "      <td>P</td>\n",
       "      <td>21.8</td>\n",
       "      <td>P</td>\n",
       "      <td>...</td>\n",
       "      <td>7.7</td>\n",
       "      <td>P</td>\n",
       "      <td>7.9</td>\n",
       "      <td>P</td>\n",
       "      <td>7.9</td>\n",
       "      <td>P</td>\n",
       "      <td>6.6</td>\n",
       "      <td>P</td>\n",
       "      <td>7.3</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39976 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           agency_cd   site_no 214956_63680_00001 214956_63680_00001_cd   \n",
       "datetime                                                                  \n",
       "20d               5s       15s                14n                   10s  \\\n",
       "1914-01-01      USGS  04024000                NaN                   NaN   \n",
       "1914-01-02      USGS  04024000                NaN                   NaN   \n",
       "1914-01-03      USGS  04024000                NaN                   NaN   \n",
       "1914-01-04      USGS  04024000                NaN                   NaN   \n",
       "...              ...       ...                ...                   ...   \n",
       "2023-06-08      USGS   4024000                3.2                     P   \n",
       "2023-06-09      USGS   4024000                3.0                 P:[4]   \n",
       "2023-06-10      USGS   4024000                2.3                     P   \n",
       "2023-06-11      USGS   4024000                2.3                     P   \n",
       "2023-06-12      USGS   4024000                2.7                     P   \n",
       "\n",
       "           214957_63680_00002 214957_63680_00002_cd 214958_63680_00003   \n",
       "datetime                                                                 \n",
       "20d                       14n                   10s                14n  \\\n",
       "1914-01-01                NaN                   NaN                NaN   \n",
       "1914-01-02                NaN                   NaN                NaN   \n",
       "1914-01-03                NaN                   NaN                NaN   \n",
       "1914-01-04                NaN                   NaN                NaN   \n",
       "...                       ...                   ...                ...   \n",
       "2023-06-08                2.0                     P                2.3   \n",
       "2023-06-09                1.9                 P:[4]                2.1   \n",
       "2023-06-10                1.8                     P                2.0   \n",
       "2023-06-11                1.6                     P                1.9   \n",
       "2023-06-12                1.7                     P                2.1   \n",
       "\n",
       "           214958_63680_00003_cd 72609_00010_00001 72609_00010_00001_cd  ...   \n",
       "datetime                                                                 ...   \n",
       "20d                          10s               14n                  10s  ...  \\\n",
       "1914-01-01                   NaN               NaN                  NaN  ...   \n",
       "1914-01-02                   NaN               NaN                  NaN  ...   \n",
       "1914-01-03                   NaN               NaN                  NaN  ...   \n",
       "1914-01-04                   NaN               NaN                  NaN  ...   \n",
       "...                          ...               ...                  ...  ...   \n",
       "2023-06-08                     P              23.8                    P  ...   \n",
       "2023-06-09                 P:[4]              22.5                P:[4]  ...   \n",
       "2023-06-10                     P              22.3                    P  ...   \n",
       "2023-06-11                     P              22.2                    P  ...   \n",
       "2023-06-12                     P              21.8                    P  ...   \n",
       "\n",
       "           72617_00400_00002 72617_00400_00002_cd 72618_00400_00008   \n",
       "datetime                                                              \n",
       "20d                      14n                  10s               14n  \\\n",
       "1914-01-01               NaN                  NaN               NaN   \n",
       "1914-01-02               NaN                  NaN               NaN   \n",
       "1914-01-03               NaN                  NaN               NaN   \n",
       "1914-01-04               NaN                  NaN               NaN   \n",
       "...                      ...                  ...               ...   \n",
       "2023-06-08               7.7                    P               7.8   \n",
       "2023-06-09               7.8                P:[4]               7.9   \n",
       "2023-06-10               7.8                    P               7.9   \n",
       "2023-06-11               7.7                    P               7.9   \n",
       "2023-06-12               7.7                    P               7.9   \n",
       "\n",
       "           72618_00400_00008_cd 72619_00300_00001 72619_00300_00001_cd   \n",
       "datetime                                                                 \n",
       "20d                         10s               14n                  10s  \\\n",
       "1914-01-01                  NaN               NaN                  NaN   \n",
       "1914-01-02                  NaN               NaN                  NaN   \n",
       "1914-01-03                  NaN               NaN                  NaN   \n",
       "1914-01-04                  NaN               NaN                  NaN   \n",
       "...                         ...               ...                  ...   \n",
       "2023-06-08                    P               7.9                    P   \n",
       "2023-06-09                P:[4]               8.0                P:[4]   \n",
       "2023-06-10                    P               7.7                    P   \n",
       "2023-06-11                    P               7.8                    P   \n",
       "2023-06-12                    P               7.9                    P   \n",
       "\n",
       "           72620_00300_00002 72620_00300_00002_cd 72621_00300_00003   \n",
       "datetime                                                              \n",
       "20d                      14n                  10s               14n  \\\n",
       "1914-01-01               NaN                  NaN               NaN   \n",
       "1914-01-02               NaN                  NaN               NaN   \n",
       "1914-01-03               NaN                  NaN               NaN   \n",
       "1914-01-04               NaN                  NaN               NaN   \n",
       "...                      ...                  ...               ...   \n",
       "2023-06-08               6.1                    P               7.0   \n",
       "2023-06-09               6.8                P:[4]               7.4   \n",
       "2023-06-10               7.1                    P               7.4   \n",
       "2023-06-11               6.0                    P               7.3   \n",
       "2023-06-12               6.6                    P               7.3   \n",
       "\n",
       "           72621_00300_00003_cd  \n",
       "datetime                         \n",
       "20d                         10s  \n",
       "1914-01-01                  NaN  \n",
       "1914-01-02                  NaN  \n",
       "1914-01-03                  NaN  \n",
       "1914-01-04                  NaN  \n",
       "...                         ...  \n",
       "2023-06-08                    P  \n",
       "2023-06-09                P:[4]  \n",
       "2023-06-10                    P  \n",
       "2023-06-11                    P  \n",
       "2023-06-12                    P  \n",
       "\n",
       "[39976 rows x 34 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "river_flow = pd.read_csv(\n",
    "    BytesIO(nwis_data.content),\n",
    "    comment='#',\n",
    "    delimiter='\\t', \n",
    "    #skiprows=[30, 31],\n",
    "    #names=['Agency', 'Site Number', 'Date', 'Discharge', 'Site Code', 'Site Code 2', 'Site Code 3'],\n",
    "    index_col='datetime',\n",
    "    parse_dates=True,\n",
    ")\n",
    "river_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39976 entries, 0 to 39975\n",
      "Data columns (total 35 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   agency_cd              39976 non-null  object\n",
      " 1   site_no                39976 non-null  object\n",
      " 2   datetime               39976 non-null  object\n",
      " 3   214956_63680_00001     4338 non-null   object\n",
      " 4   214956_63680_00001_cd  4338 non-null   object\n",
      " 5   214957_63680_00002     4340 non-null   object\n",
      " 6   214957_63680_00002_cd  4340 non-null   object\n",
      " 7   214958_63680_00003     4306 non-null   object\n",
      " 8   214958_63680_00003_cd  4306 non-null   object\n",
      " 9   72609_00010_00001      5311 non-null   object\n",
      " 10  72609_00010_00001_cd   5311 non-null   object\n",
      " 11  72610_00010_00002      4617 non-null   object\n",
      " 12  72610_00010_00002_cd   4617 non-null   object\n",
      " 13  72611_00010_00003      4996 non-null   object\n",
      " 14  72611_00010_00003_cd   4996 non-null   object\n",
      " 15  72612_00095_00001      4794 non-null   object\n",
      " 16  72612_00095_00001_cd   4794 non-null   object\n",
      " 17  72613_00095_00002      5154 non-null   object\n",
      " 18  72613_00095_00002_cd   5154 non-null   object\n",
      " 19  72614_00095_00003      4750 non-null   object\n",
      " 20  72614_00095_00003_cd   4750 non-null   object\n",
      " 21  72615_00060_00003      39964 non-null  object\n",
      " 22  72615_00060_00003_cd   39964 non-null  object\n",
      " 23  72616_00400_00001      4347 non-null   object\n",
      " 24  72616_00400_00001_cd   4347 non-null   object\n",
      " 25  72617_00400_00002      4344 non-null   object\n",
      " 26  72617_00400_00002_cd   4344 non-null   object\n",
      " 27  72618_00400_00008      4344 non-null   object\n",
      " 28  72618_00400_00008_cd   4344 non-null   object\n",
      " 29  72619_00300_00001      4366 non-null   object\n",
      " 30  72619_00300_00001_cd   4366 non-null   object\n",
      " 31  72620_00300_00002      4369 non-null   object\n",
      " 32  72620_00300_00002_cd   4369 non-null   object\n",
      " 33  72621_00300_00003      4366 non-null   object\n",
      " 34  72621_00300_00003_cd   4366 non-null   object\n",
      "dtypes: object(35)\n",
      "memory usage: 10.7+ MB\n"
     ]
    }
   ],
   "source": [
    "river_flow.info()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "248.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
